<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AgriVision AI ‚Ä¢ Mobile Optimized</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: #000; color: white; 
            height: 100vh; overflow: hidden;
            -webkit-tap-highlight-color: transparent;
        }
        .container { 
            height: 100vh; 
            display: flex; 
            flex-direction: column;
            position: relative;
        }
        .header {
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(0, 0, 0, 0.7);
            backdrop-filter: blur(10px);
            padding: 12px 16px;
            z-index: 100;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }
        .status {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .status-indicator {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #4CAF50;
            animation: pulse 2s infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        .status-text {
            font-size: 14px;
            font-weight: 500;
        }
        .video-container {
            flex: 1;
            position: relative;
            background: #000;
            overflow: hidden;
        }
        #video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1);
        }
        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            transform: scaleX(-1);
        }
        .controls {
            position: absolute;
            bottom: 20px;
            left: 0;
            right: 0;
            display: flex;
            justify-content: center;
            gap: 15px;
            padding: 0 20px;
            z-index: 50;
        }
        .btn {
            background: rgba(255, 255, 255, 0.15);
            border: 1px solid rgba(255, 255, 255, 0.2);
            color: white;
            padding: 12px 24px;
            border-radius: 12px;
            font-size: 14px;
            font-weight: 600;
            backdrop-filter: blur(10px);
            cursor: pointer;
            transition: all 0.2s;
        }
        .btn:active {
            transform: scale(0.95);
            background: rgba(255, 255, 255, 0.25);
        }
        .btn-primary {
            background: #4CAF50;
            border-color: #4CAF50;
        }
        .stats {
            position: absolute;
            bottom: 80px;
            left: 0;
            right: 0;
            display: flex;
            justify-content: center;
            gap: 20px;
            padding: 0 20px;
        }
        .stat-box {
            background: rgba(0, 0, 0, 0.7);
            backdrop-filter: blur(10px);
            border-radius: 12px;
            padding: 15px;
            min-width: 90px;
            text-align: center;
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        .stat-value {
            font-size: 28px;
            font-weight: bold;
            margin-bottom: 5px;
        }
        .stat-label {
            font-size: 12px;
            opacity: 0.8;
        }
        .focus-overlay {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 200px;
            height: 200px;
            border: 2px dashed rgba(255, 255, 255, 0.5);
            border-radius: 20px;
            pointer-events: none;
            opacity: 0;
            transition: opacity 0.3s;
        }
        .debug {
            position: absolute;
            top: 60px;
            left: 10px;
            background: rgba(0, 0, 0, 0.7);
            color: #00ff9d;
            padding: 8px 12px;
            border-radius: 8px;
            font-size: 11px;
            font-family: monospace;
            z-index: 1000;
            display: none;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="status">
                <div class="status-indicator"></div>
                <div class="status-text" id="status">Initializing camera...</div>
            </div>
        </div>
        
        <div class="video-container">
            <video id="video" autoplay playsinline></video>
            <canvas id="canvas"></canvas>
            <div class="focus-overlay" id="focusOverlay"></div>
            <div class="debug" id="debug"></div>
        </div>
        
        <div class="stats">
            <div class="stat-box">
                <div class="stat-value" id="cropCount">0</div>
                <div class="stat-label">CROPS</div>
            </div>
            <div class="stat-box">
                <div class="stat-value" id="weedCount">0</div>
                <div class="stat-label">WEEDS</div>
            </div>
        </div>
        
        <div class="controls">
            <button class="btn" id="focusBtn">üîç Tap Focus</button>
            <button class="btn btn-primary" id="toggleBtn">‚ñ∂Ô∏è Start Detection</button>
        </div>
    </div>

    <script>
        // DOM Elements
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusEl = document.getElementById('status');
        const cropCountEl = document.getElementById('cropCount');
        const weedCountEl = document.getElementById('weedCount');
        const focusBtn = document.getElementById('focusBtn');
        const toggleBtn = document.getElementById('toggleBtn');
        const focusOverlay = document.getElementById('focusOverlay');
        const debugEl = document.getElementById('debug');

        // State variables
        let model = null;
        let isDetecting = false;
        let detectionActive = false;
        let videoTrack = null;
        let cropCount = 0;
        let weedCount = 0;
        let frameCounter = 0;
        let animationId = null;

        // Configuration
        const CONFIDENCE_THRESHOLD = 0.65; // Increased threshold to prevent false positives
        const IOU_THRESHOLD = 0.5;
        const MODEL_INPUT_SIZE = 640;

        // ==================== MOBILE CAMERA SETUP ====================
        async function initCamera() {
            try {
                statusEl.textContent = 'Accessing camera...';
                
                // SIMPLIFIED constraints that work on mobile
                const constraints = {
                    video: {
                        width: { ideal: 1280 },
                        height: { ideal: 720 },
                        facingMode: { ideal: 'environment' }, // Back camera
                        frameRate: { ideal: 30 }
                    },
                    audio: false
                };

                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                videoTrack = stream.getVideoTracks()[0];

                // Wait for video to be ready
                await new Promise((resolve) => {
                    video.onloadedmetadata = () => {
                        video.play().then(resolve);
                    };
                });

                // Set canvas size to match video
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;

                // Enable tap-to-focus
                setupTapToFocus();

                statusEl.textContent = 'Camera ready - Tap "Start Detection"';
                console.log('Camera initialized:', video.videoWidth, 'x', video.videoHeight);

            } catch (error) {
                console.error('Camera error:', error);
                statusEl.textContent = 'Camera error: ' + error.message;
                
                // Fallback to basic constraints
                try {
                    const basicConstraints = { video: true, audio: false };
                    const stream = await navigator.mediaDevices.getUserMedia(basicConstraints);
                    video.srcObject = stream;
                    video.play();
                    statusEl.textContent = 'Camera ready (basic mode)';
                } catch (fallbackError) {
                    statusEl.textContent = 'Cannot access camera. Please check permissions.';
                }
            }
        }

        // ==================== TAP-TO-FOCUS ====================
        function setupTapToFocus() {
            if (!videoTrack || !videoTrack.getCapabilities) return;

            video.addEventListener('click', async (e) => {
                const rect = video.getBoundingClientRect();
                const x = (e.clientX - rect.left) / rect.width;
                const y = (e.clientY - rect.top) / rect.height;

                // Show focus overlay
                focusOverlay.style.opacity = '1';
                focusOverlay.style.left = (x * 100) + '%';
                focusOverlay.style.top = (y * 100) + '%';

                try {
                    // Try to set focus point (works on modern mobile browsers)
                    await videoTrack.applyConstraints({
                        advanced: [{
                            focusMode: 'manual',
                            focusDistance: 0.5
                        }]
                    });

                    // Flash effect
                    setTimeout(() => {
                        focusOverlay.style.opacity = '0';
                    }, 800);

                    // Revert to auto focus after delay
                    setTimeout(() => {
                        if (videoTrack.getCapabilities().focusMode && 
                            videoTrack.getCapabilities().focusMode.includes('continuous')) {
                            videoTrack.applyConstraints({
                                advanced: [{ focusMode: 'continuous' }]
                            });
                        }
                    }, 3000);

                } catch (focusError) {
                    console.log('Tap focus not supported on this device');
                    focusOverlay.style.opacity = '0';
                }
            });
        }

        // ==================== MODEL LOADING ====================
        async function loadModel() {
            try {
                statusEl.textContent = 'Loading AI model...';
                
                // Try multiple model paths
                const modelPaths = [
                    'model.json',
                    './model.json',
                    'best_model_web/model.json',
                    'weights/model.json'
                ];

                for (const path of modelPaths) {
                    try {
                        console.log('Trying model path:', path);
                        model = await tf.loadGraphModel(path);
                        console.log('‚úÖ Model loaded from:', path);
                        console.log('Model input shape:', model.inputs[0].shape);
                        statusEl.textContent = 'Model loaded successfully';
                        return true;
                    } catch (e) {
                        console.log('Failed from', path, e.message);
                    }
                }

                throw new Error('Could not load model from any path');

            } catch (error) {
                console.error('Model load failed:', error);
                statusEl.textContent = 'Model load failed - Using simulated detection';
                return false;
            }
        }

        // ==================== DETECTION LOGIC ====================
        function processModelOutput(prediction) {
            // Clear previous drawings
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            // Reset counts for this frame
            let frameCrops = 0;
            let frameWeeds = 0;

            try {
                // Get the prediction data - handle different model output formats
                let data;
                if (Array.isArray(prediction)) {
                    data = prediction[0].dataSync();
                } else {
                    data = prediction.dataSync();
                }

                // Process detections
                // Assuming model outputs [x, y, width, height, confidence, class]
                const numDetections = data.length / 6;
                
                for (let i = 0; i < numDetections; i++) {
                    const offset = i * 6;
                    const x = data[offset] * canvas.width;
                    const y = data[offset + 1] * canvas.height;
                    const width = data[offset + 2] * canvas.width;
                    const height = data[offset + 3] * canvas.height;
                    const confidence = data[offset + 4];
                    const classId = Math.round(data[offset + 5]);

                    // Apply confidence threshold
                    if (confidence < CONFIDENCE_THRESHOLD) continue;
                    if (width < 20 || height < 20) continue; // Minimum size filter

                    // Scale coordinates
                    const scaleX = canvas.width / MODEL_INPUT_SIZE;
                    const scaleY = canvas.height / MODEL_INPUT_SIZE;
                    
                    const scaledX = x * scaleX;
                    const scaledY = y * scaleY;
                    const scaledWidth = width * scaleX;
                    const scaledHeight = height * scaleY;

                    // Draw bounding box
                    ctx.beginPath();
                    ctx.lineWidth = 3;
                    
                    if (classId === 0) { // Weed
                        ctx.strokeStyle = '#ff4444';
                        ctx.fillStyle = 'rgba(255, 68, 68, 0.2)';
                        frameWeeds++;
                    } else { // Crop
                        ctx.strokeStyle = '#44aa44';
                        ctx.fillStyle = 'rgba(68, 170, 68, 0.2)';
                        frameCrops++;
                    }
                    
                    ctx.rect(scaledX, scaledY, scaledWidth, scaledHeight);
                    ctx.stroke();
                    ctx.fill();

                    // Draw label
                    const label = classId === 0 ? `WEED ${(confidence * 100).toFixed(0)}%` 
                                               : `CROP ${(confidence * 100).toFixed(0)}%`;
                    ctx.fillStyle = 'rgba(0, 0, 0, 0.7)';
                    ctx.fillRect(scaledX, scaledY - 20, 120, 20);
                    ctx.fillStyle = 'white';
                    ctx.font = '12px Arial';
                    ctx.fillText(label, scaledX + 5, scaledY - 5);
                }

                // Only update counts if we actually found something
                if (frameCrops > 0 || frameWeeds > 0) {
                    cropCount = frameCrops;
                    weedCount = frameWeeds;
                    updateCounts();
                }

                return { crops: frameCrops, weeds: frameWeeds };

            } catch (error) {
                console.error('Error processing model output:', error);
                return { crops: 0, weeds: 0 };
            }
        }

        // ==================== DETECTION LOOP ====================
        async function detectFrame() {
            if (!detectionActive || !isDetecting) return;

            frameCounter++;
            
            try {
                // Convert video frame to tensor
                const img = tf.browser.fromPixels(video);
                const resized = tf.image.resizeBilinear(img, [MODEL_INPUT_SIZE, MODEL_INPUT_SIZE]);
                const normalized = resized.div(255.0);
                const batched = normalized.expandDims(0);

                // Run inference
                let prediction;
                if (model) {
                    try {
                        prediction = model.execute(batched);
                    } catch (e) {
                        prediction = await model.executeAsync(batched);
                    }

                    // Process results
                    const results = processModelOutput(prediction);
                    
                    // Update status
                    if (results.crops > 0 || results.weeds > 0) {
                        statusEl.textContent = `Detected: ${results.crops} crops, ${results.weeds} weeds`;
                    } else {
                        statusEl.textContent = 'Scanning... (move closer)';
                    }

                    // Clean up tensors
                    tf.dispose([img, resized, normalized, batched, prediction]);
                } else {
                    // Simulated detection for testing
                    simulateDetection();
                }

            } catch (error) {
                console.error('Detection error:', error);
                statusEl.textContent = 'Detection error - retrying';
            }

            // Continue detection loop
            if (detectionActive) {
                setTimeout(() => {
                    animationId = requestAnimationFrame(detectFrame);
                }, 100); // ~10 FPS for performance
            }
        }

        // ==================== SIMULATION (for testing) ====================
        function simulateDetection() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            
            // Only simulate sometimes to avoid false counts
            if (Math.random() > 0.7) {
                const x = Math.random() * canvas.width * 0.8;
                const y = Math.random() * canvas.height * 0.8;
                const size = 50 + Math.random() * 100;
                
                if (Math.random() > 0.5) {
                    // Simulate weed
                    ctx.strokeStyle = '#ff4444';
                    ctx.fillStyle = 'rgba(255, 68, 68, 0.2)';
                    weedCount++;
                } else {
                    // Simulate crop
                    ctx.strokeStyle = '#44aa44';
                    ctx.fillStyle = 'rgba(68, 170, 68, 0.2)';
                    cropCount++;
                }
                
                ctx.lineWidth = 3;
                ctx.rect(x, y, size, size);
                ctx.stroke();
                ctx.fill();
                
                updateCounts();
                statusEl.textContent = `Simulated detection (${cropCount} crops, ${weedCount} weeds)`;
            }
        }

        // ==================== UTILITY FUNCTIONS ====================
        function updateCounts() {
            cropCountEl.textContent = cropCount;
            weedCountEl.textContent = weedCount;
        }

        function resetCounts() {
            cropCount = 0;
            weedCount = 0;
            updateCounts();
            ctx.clearRect(0, 0, canvas.width, canvas.height);
        }

        // ==================== EVENT HANDLERS ====================
        focusBtn.addEventListener('click', () => {
            // Trigger focus at center
            focusOverlay.style.opacity = '1';
            focusOverlay.style.left = '50%';
            focusOverlay.style.top = '50%';
            
            setTimeout(() => {
                focusOverlay.style.opacity = '0';
            }, 800);
            
            statusEl.textContent = 'Focus adjusted';
        });

        toggleBtn.addEventListener('click', async () => {
            if (!isDetecting) {
                // Start detection
                if (!model) {
                    const loaded = await loadModel();
                    if (!loaded) {
                        statusEl.textContent = 'Using simulated detection (no model)';
                    }
                }
                
                isDetecting = true;
                detectionActive = true;
                toggleBtn.textContent = '‚è∏Ô∏è Stop Detection';
                toggleBtn.style.background = '#ff4444';
                statusEl.textContent = 'Detection started - Scanning...';
                
                // Reset counts when starting
                resetCounts();
                
                // Start detection loop
                detectFrame();
                
            } else {
                // Stop detection
                detectionActive = false;
                isDetecting = false;
                toggleBtn.textContent = '‚ñ∂Ô∏è Start Detection';
                toggleBtn.style.background = '#4CAF50';
                statusEl.textContent = 'Detection stopped';
                
                if (animationId) {
                    cancelAnimationFrame(animationId);
                }
            }
        });

        // ==================== INITIALIZATION ====================
        async function init() {
            console.log('Initializing AgriVision AI...');
            
            // Initialize camera
            await initCamera();
            
            // Preload model in background
            loadModel().then(loaded => {
                if (loaded) {
                    console.log('Model ready in background');
                }
            });
            
            // Update status
            statusEl.textContent = 'Ready - Tap "Start Detection" to begin';
            
            // Show debug info on long press
            let debugTimeout;
            video.addEventListener('touchstart', () => {
                debugTimeout = setTimeout(() => {
                    debugEl.style.display = 'block';
                    debugEl.textContent = `Size: ${video.videoWidth}x${video.videoHeight}\nFrame: ${frameCounter}`;
                }, 2000);
            });
            
            video.addEventListener('touchend', () => {
                clearTimeout(debugTimeout);
                setTimeout(() => {
                    debugEl.style.display = 'none';
                }, 3000);
            });
        }

        // Start the app
        init();
    </script>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>
    <title>AgriVision AI ‚Ä¢ Professional Weed & Crop Detection</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Inter', system-ui, sans-serif; background: #0a0a0a; color: #e0e0e0; min-height: 100vh; overflow: hidden; }
        .container { height: 100vh; display: flex; flex-direction: column; position: relative; background: linear-gradient(135deg, #0f0f0f 0%, #000 100%); }
        .status-bar { position: absolute; top: 16px; left: 16px; right: 16px; z-index: 100; background: rgba(20,20,20,0.6); backdrop-filter: blur(12px); border-radius: 16px; padding: 12px 16px; border: 1px solid rgba(255,255,255,0.08); display: flex; align-items: center; gap: 12px; box-shadow: 0 8px 32px rgba(0,0,0,0.4); }
        .status-dot { width: 10px; height: 10px; border-radius: 50%; background: #00ff9d; box-shadow: 0 0 12px #00ff9d88; }
        .status-text { flex: 1; }
        .status-title { font-size: 14px; font-weight: 600; color: #00ff9d; }
        .status-desc { font-size: 12px; opacity: 0.7; margin-top: 2px; }
        .frame-count { background: rgba(255,255,255,0.08); padding: 6px 12px; border-radius: 8px; font-size: 12px; font-weight: 500; }
        .detection-area { flex: 1; position: relative; overflow: hidden; }
        #video { width: 100%; height: 100%; object-fit: cover; background: #000; }
        canvas { position: absolute; inset: 0; pointer-events: none; }
        .bottom-panel-wrapper { position: absolute; bottom: 0; left: 0; right: 0; z-index: 50; transition: transform 0.4s cubic-bezier(0.22, 1, 0.36, 1); }
        .bottom-panel { background: rgba(15,15,15,0.92); backdrop-filter: blur(20px); border-top-left-radius: 24px; border-top-right-radius: 24px; border-top: 1px solid rgba(255,255,255,0.08); box-shadow: 0 -8px 40px rgba(0,0,0,0.6); padding: 0 20px 20px; touch-action: pan-y; }
        .drag-handle { width: 40px; height: 4px; background: rgba(255,255,255,0.3); border-radius: 2px; margin: 12px auto; cursor: grab; }
        .stats-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 16px; margin: 20px 0; }
        .stat-card { background: rgba(255,255,255,0.05); border-radius: 16px; padding: 16px; text-align: center; border: 1px solid rgba(255,255,255,0.08); transition: all 0.3s ease; }
        .stat-card:hover { transform: translateY(-4px); background: rgba(255,255,255,0.08); }
        .stat-icon { font-size: 28px; margin-bottom: 8px; }
        .stat-value { font-size: 28px; font-weight: 700; margin: 4px 0; }
        .stat-label { font-size: 13px; opacity: 0.7; }
        .confidence-bar { height: 8px; background: rgba(255,255,255,0.1); border-radius: 4px; overflow: hidden; margin: 12px 0; }
        .confidence-fill { height: 100%; background: linear-gradient(90deg, #00ff9d, #4CAF50); transition: width 0.6s ease; }
        .bottom-actions { display: flex; justify-content: space-around; margin-top: 16px; }
        .action-btn { background: rgba(255,255,255,0.08); border: 1px solid rgba(255,255,255,0.15); color: white; padding: 12px 24px; border-radius: 12px; font-weight: 500; cursor: pointer; transition: all 0.3s; }
        .action-btn:hover { background: rgba(255,255,255,0.15); transform: translateY(-2px); }
        .primary-btn { background: #4CAF50; border-color: #4CAF50; }
        .primary-btn:hover { background: #43A047; }
        @media (max-width: 600px) { .stats-grid { gap: 12px; } .stat-value { font-size: 22px; } .action-btn { padding: 10px 16px; font-size: 14px; } }
    </style>
</head>
<body>
    <div class="container">
        <div class="status-bar">
            <div class="status-dot"></div>
            <div class="status-text">
                <p class="status-title" id="detection-title">Live Detection Active</p>
                <p class="status-desc" id="status-desc">Loading model...</p>
            </div>
            <div class="frame-count">Frame: <span id="frame-count">0</span></div>
        </div>

        <div class="detection-area">
            <video id="video" autoplay playsinline muted></video>
            <canvas id="canvas"></canvas>
        </div>

        <div class="bottom-panel-wrapper" id="bottom-wrapper">
            <div class="bottom-panel">
                <div class="drag-handle" id="drag-handle"></div>

                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-icon">üåø</div>
                        <div class="stat-value" id="crop-count">0</div>
                        <div class="stat-label">Crops</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-icon">‚ö†Ô∏è</div>
                        <div class="stat-value" id="weed-count">0</div>
                        <div class="stat-label">Weeds</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-icon">üìä</div>
                        <div class="stat-value" id="total-count">0</div>
                        <div class="stat-label">Total</div>
                    </div>
                </div>

                <div class="confidence-bar">
                    <div class="confidence-fill" id="confidence-fill" style="width: 0%"></div>
                </div>
                <div class="confidence-label" id="confidence-label" style="text-align: center; font-size: 12px; margin-top: 4px;">Avg Confidence: 0%</div>

                <div class="bottom-actions">
                    <button class="action-btn" id="pause-btn">Pause Detection</button>
                    <button class="action-btn primary-btn" id="reset-btn">Reset Counts</button>
                </div>
            </div>
        </div>
    </div>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusDesc = document.getElementById('status-desc');
        const frameCount = document.getElementById('frame-count');
        const cropCount = document.getElementById('crop-count');
        const weedCount = document.getElementById('weed-count');
        const totalCount = document.getElementById('total-count');
        const confidenceFill = document.getElementById('confidence-fill');
        const confidenceLabel = document.getElementById('confidence-label');
        const pauseBtn = document.getElementById('pause-btn');
        const resetBtn = document.getElementById('reset-btn');
        const bottomWrapper = document.getElementById('bottom-wrapper');
        const dragHandle = document.getElementById('drag-handle');

        let model;
        let isDetecting = true;
        let frameNum = 0;
        let totalWeeds = 0;
        let totalCrops = 0;
        let avgConf = 0;
        let animationId = null;

        // Tracking IDs
        let weedId = 0;
        let cropId = 0;
        const seenObjects = new Set();
        const objectLabels = new Map();

        // Enhanced camera constraints with better focus
        const constraints = {
            video: {
                facingMode: { ideal: 'environment' },
                width: { min: 1280, ideal: 1920, max: 2560 },
                height: { min: 720, ideal: 1080, max: 1440 },
                frameRate: { ideal: 30, min: 24 },
                aspectRatio: { ideal: 16/9 }
            },
            audio: false
        };

        // Confidence and IOU thresholds
        const CONFIDENCE_THRESHOLD = 0.7;
        const IOU_THRESHOLD = 0.6;

        function resizeCanvas() {
            if (video.videoWidth && video.videoHeight) {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
            }
        }

        async function startCamera() {
            try {
                console.log('Requesting camera...');
                
                // Try to get back camera
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(device => device.kind === 'videoinput');
                
                let stream;
                let backCameraFound = false;
                
                // Try to find and use back camera
                for (const device of videoDevices) {
                    if (device.label.toLowerCase().includes('back') || 
                        device.label.toLowerCase().includes('rear')) {
                        console.log('Found back camera:', device.label);
                        
                        const backConstraints = {
                            video: {
                                deviceId: { exact: device.deviceId },
                                width: { ideal: 1920 },
                                height: { ideal: 1080 },
                                frameRate: { ideal: 30 }
                            }
                        };
                        
                        try {
                            stream = await navigator.mediaDevices.getUserMedia(backConstraints);
                            backCameraFound = true;
                            break;
                        } catch (e) {
                            console.log('Back camera failed, trying next...');
                        }
                    }
                }
                
                // If back camera not found or failed, use default
                if (!backCameraFound) {
                    console.log('Using default camera');
                    stream = await navigator.mediaDevices.getUserMedia(constraints);
                }
                
                video.srcObject = stream;
                
                // Apply focus constraints if supported
                const videoTrack = stream.getVideoTracks()[0];
                if (videoTrack.getCapabilities().focusMode) {
                    try {
                        await videoTrack.applyConstraints({
                            advanced: [{ focusMode: 'continuous' }]
                        });
                        console.log('Continuous focus enabled');
                    } catch (e) {
                        console.log('Focus constraint not fully supported:', e);
                    }
                }
                
                // Try to optimize video quality
                try {
                    await videoTrack.applyConstraints({
                        advanced: [
                            { focusMode: 'continuous' },
                            { exposureMode: 'continuous' },
                            { whiteBalanceMode: 'continuous' }
                        ]
                    });
                } catch (e) {
                    console.log('Some constraints not supported');
                }

                video.onloadedmetadata = () => {
                    console.log('Video dimensions:', video.videoWidth, 'x', video.videoHeight);
                    video.play();
                    resizeCanvas();
                    
                    // Add tap-to-focus functionality
                    canvas.addEventListener('click', (e) => {
                        const rect = canvas.getBoundingClientRect();
                        const x = (e.clientX - rect.left) / rect.width;
                        const y = (e.clientY - rect.top) / rect.height;
                        
                        if (videoTrack.getCapabilities && videoTrack.getCapabilities().focusDistance) {
                            try {
                                videoTrack.applyConstraints({
                                    advanced: [{ focusMode: 'manual', focusDistance: 0.5 }]
                                }).then(() => {
                                    console.log('Manual focus applied');
                                    setTimeout(() => {
                                        videoTrack.applyConstraints({
                                            advanced: [{ focusMode: 'continuous' }]
                                        });
                                    }, 3000);
                                });
                            } catch (e) {
                                console.log('Tap to focus failed');
                            }
                        }
                    });
                    
                    statusDesc.textContent = 'High-quality camera ready';
                    loadModel();
                };
                
                window.addEventListener('resize', resizeCanvas);
                
            } catch (e) {
                console.error('Camera error:', e);
                statusDesc.textContent = 'Camera failed - trying basic mode';
                
                // Try basic constraints as fallback
                try {
                    const basicConstraints = {
                        video: {
                            facingMode: 'environment',
                            width: { ideal: 1280 },
                            height: { ideal: 720 }
                        },
                        audio: false
                    };
                    
                    const stream = await navigator.mediaDevices.getUserMedia(basicConstraints);
                    video.srcObject = stream;
                    video.play();
                    loadModel();
                } catch (fallbackError) {
                    statusDesc.textContent = 'Cannot access camera. Please allow camera permissions.';
                    console.error('Fallback also failed:', fallbackError);
                }
            }
        }

        async function loadModel() {
            try {
                // Try different model paths
                const modelPaths = [
                    'best_web_model/model.json',
                    'model.json',
                    './model.json',
                    'best/model.json'
                ];
                
                let modelLoaded = false;
                for (const path of modelPaths) {
                    try {
                        console.log(`Trying to load model from: ${path}`);
                        model = await tf.loadGraphModel(path);
                        console.log('‚úÖ Model loaded successfully from:', path);
                        modelLoaded = true;
                        break;
                    } catch (e) {
                        console.log(`Failed to load from ${path}:`, e.message);
                    }
                }
                
                if (!modelLoaded) {
                    throw new Error('Could not load model from any path');
                }
                
                statusDesc.textContent = 'High-precision model loaded ‚Äì detecting with 70%+ confidence';
                detectFrame();
            } catch (e) {
                console.error('Model load error:', e);
                statusDesc.textContent = 'Model load failed ‚Äì check console';
            }
        }

        // Fixed NMS function
        function nonMaxSuppression(boxes, scores, iouThreshold = IOU_THRESHOLD) {
            const indices = [];
            let remaining = scores.map((s, i) => ({score: s, index: i}))
                                  .sort((a, b) => b.score - a.score);

            while (remaining.length > 0) {
                const best = remaining.shift();
                indices.push(best.index);

                remaining = remaining.filter(item => {
                    const iou = calculateIOU(boxes[best.index], boxes[item.index]);
                    return iou <= iouThreshold;
                });
            }
            return indices;
        }

        function calculateIOU(box1, box2) {
            const [x1, y1, w1, h1] = box1;
            const [x2, y2, w2, h2] = box2;

            const xi1 = Math.max(x1, x2);
            const yi1 = Math.max(y1, y2);
            const xi2 = Math.min(x1 + w1, x2 + w2);
            const yi2 = Math.min(y1 + h1, y2 + h2);

            const interArea = Math.max(0, xi2 - xi1) * Math.max(0, yi2 - yi1);
            const box1Area = w1 * h1;
            const box2Area = w2 * h2;
            const unionArea = box1Area + box2Area - interArea;
            
            return unionArea > 0 ? interArea / unionArea : 0;
        }

        async function detectFrame() {
            if (!isDetecting || !model || !video.videoWidth) {
                animationId = requestAnimationFrame(detectFrame);
                return;
            }

            frameNum++;
            frameCount.textContent = frameNum;

            tf.engine().startScope();

            try {
                // Capture frame
                const img = tf.browser.fromPixels(video);
                const resized = tf.image.resizeBilinear(img, [640, 640]);
                const normalized = resized.div(255.0);
                const batched = normalized.expandDims(0);

                // Use model.execute() for better performance
                let prediction;
                try {
                    prediction = model.execute(batched);
                } catch (executeError) {
                    console.log('Sync execution failed, trying async:', executeError.message);
                    prediction = await model.executeAsync(batched);
                }

                // Handle prediction output
                let output = Array.isArray(prediction) ? prediction[0] : prediction;
                
                // Process based on shape
                const shape = output.shape;
                const data = await output.data();
                
                const rawDetections = [];
                console.log(`Model output shape: ${shape}`); // Debug log
                
                if (shape[1] === 6 && shape[2] === 8400) {
                    // Shape: [1, 6, 8400] - standard YOLO format
                    for (let i = 0; i < 8400; i++) {
                        const cx = data[i];
                        const cy = data[8400 + i];
                        const w = data[16800 + i];
                        const h = data[25200 + i];
                        const weedConf = data[33600 + i];
                        const cropConf = data[42000 + i];

                        const conf = Math.max(weedConf, cropConf);
                        if (conf < CONFIDENCE_THRESHOLD) continue;

                        const classId = weedConf > cropConf ? 0 : 1;
                        const x = (cx - w / 2) * 640;
                        const y = (cy - h / 2) * 640;
                        const width = w * 640;
                        const height = h * 640;

                        if (width > 10 && height > 10) {
                            rawDetections.push({
                                bbox: [x, y, width, height],
                                confidence: conf,
                                classId
                            });
                        }
                    }
                } else if (shape[1] === 8400 && shape[2] === 6) {
                    // Shape: [1, 8400, 6] - transposed format
                    for (let i = 0; i < 8400; i++) {
                        const offset = i * 6;
                        const cx = data[offset];
                        const cy = data[offset + 1];
                        const w = data[offset + 2];
                        const h = data[offset + 3];
                        const weedConf = data[offset + 4];
                        const cropConf = data[offset + 5];

                        const conf = Math.max(weedConf, cropConf);
                        if (conf < CONFIDENCE_THRESHOLD) continue;

                        const classId = weedConf > cropConf ? 0 : 1;
                        const x = (cx - w / 2) * 640;
                        const y = (cy - h / 2) * 640;
                        const width = w * 640;
                        const height = h * 640;

                        if (width > 10 && height > 10) {
                            rawDetections.push({
                                bbox: [x, y, width, height],
                                confidence: conf,
                                classId
                            });
                        }
                    }
                } else {
                    console.warn(`Unexpected model output shape: ${shape}. Trying generic processing...`);
                    // Generic processing for unknown shape
                    const totalElements = shape.reduce((a, b) => a * b, 1);
                    console.log(`Total elements in output: ${totalElements}`);
                }

                console.log(`Raw detections before NMS: ${rawDetections.length}`); // Debug log

                // Clean up tensors to prevent memory leaks
                img.dispose();
                resized.dispose();
                normalized.dispose();
                batched.dispose();
                output.dispose();
                if (prediction && prediction.dispose) prediction.dispose();

                // Apply NMS
                if (rawDetections.length > 0) {
                    const boxes = rawDetections.map(d => d.bbox);
                    const scores = rawDetections.map(d => d.confidence);
                    const indices = nonMaxSuppression(boxes, scores, IOU_THRESHOLD);

                    const detections = indices.map(idx => {
                        const d = rawDetections[idx];
                        const isWeed = d.classId === 0;
                        const objectId = `${d.classId}_${Math.round(d.bbox[0])}_${Math.round(d.bbox[1])}_${Math.round(d.bbox[2])}_${Math.round(d.bbox[3])}`;

                        let label;
                        if (!seenObjects.has(objectId)) {
                            seenObjects.add(objectId);
                            if (isWeed) {
                                weedId++;
                                totalWeeds++;
                                label = `WEED #${weedId}`;
                            } else {
                                cropId++;
                                totalCrops++;
                                label = `CROP #${cropId}`;
                            }
                            objectLabels.set(objectId, label);
                        } else {
                            label = objectLabels.get(objectId);
                        }

                        return {
                            label,
                            confidence: d.confidence,
                            bbox: d.bbox,
                            class: isWeed ? 'weed' : 'crop'
                        };
                    });

                    console.log(`Detections after NMS: ${detections.length}`); // Debug log

                    // Update stats
                    totalCount.textContent = totalWeeds + totalCrops;
                    weedCount.textContent = totalWeeds;
                    cropCount.textContent = totalCrops;

                    if (detections.length > 0) {
                        avgConf = detections.reduce((s, d) => s + d.confidence, 0) / detections.length;
                        const confidencePercent = Math.min(100, avgConf * 100);
                        confidenceFill.style.width = `${confidencePercent}%`;
                        confidenceLabel.textContent = `Avg Confidence: ${confidencePercent.toFixed(1)}% (${detections.length} objects)`;
                        statusDesc.textContent = `High-confidence detection: ${detections.length} objects`;
                    } else {
                        statusDesc.textContent = 'Scanning for plants (70%+ confidence required)';
                    }

                    // Draw detections with proper scaling
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    
                    detections.forEach(d => {
                        // Calculate scale factors
                        const modelWidth = 640;
                        const modelHeight = 640;
                        const scaleX = video.videoWidth / modelWidth;
                        const scaleY = video.videoHeight / modelHeight;
                        
                        // Scale coordinates from model space to video space
                        const sx = d.bbox[0] * scaleX;
                        const sy = d.bbox[1] * scaleY;
                        const sw = d.bbox[2] * scaleX;
                        const sh = d.bbox[3] * scaleY;
                        
                        console.log(`Drawing box: ${sx}, ${sy}, ${sw}, ${sh}`); // Debug log

                        // Draw bounding box with glow effect
                        ctx.lineWidth = 3;
                        ctx.strokeStyle = d.class === 'weed' ? '#ff4444' : '#44aa44';
                        ctx.shadowColor = d.class === 'weed' ? '#ff4444' : '#44aa44';
                        ctx.shadowBlur = 15;
                        ctx.strokeRect(sx, sy, sw, sh);
                        ctx.shadowBlur = 0;

                        // Draw label background
                        const labelText = `${d.label} ${(d.confidence * 100).toFixed(0)}%`;
                        ctx.font = 'bold 14px Inter';
                        const textWidth = ctx.measureText(labelText).width;
                        const labelWidth = textWidth + 20;
                        
                        // Rounded rectangle for label
                        const labelX = sx;
                        const labelY = sy - 30;
                        const radius = 6;
                        
                        ctx.fillStyle = d.class === 'weed' ? '#ff4444' : '#44aa44';
                        ctx.beginPath();
                        ctx.moveTo(labelX + radius, labelY);
                        ctx.lineTo(labelX + labelWidth - radius, labelY);
                        ctx.quadraticCurveTo(labelX + labelWidth, labelY, labelX + labelWidth, labelY + radius);
                        ctx.lineTo(labelX + labelWidth, labelY + 30 - radius);
                        ctx.quadraticCurveTo(labelX + labelWidth, labelY + 30, labelX + labelWidth - radius, labelY + 30);
                        ctx.lineTo(labelX + radius, labelY + 30);
                        ctx.quadraticCurveTo(labelX, labelY + 30, labelX, labelY + 30 - radius);
                        ctx.lineTo(labelX, labelY + radius);
                        ctx.quadraticCurveTo(labelX, labelY, labelX + radius, labelY);
                        ctx.closePath();
                        ctx.fill();

                        // Draw label text
                        ctx.fillStyle = 'white';
                        ctx.font = 'bold 14px Inter';
                        ctx.fillText(labelText, labelX + 10, labelY + 20);
                    });
                } else {
                    statusDesc.textContent = 'No high-confidence detections ‚Äì move closer/improve lighting';
                }

            } catch (e) {
                console.error('Detection error:', e);
                statusDesc.textContent = 'Detection error - continuing';
            }

            tf.engine().endScope();
            
            // Adjust frame rate for performance
            setTimeout(() => {
                animationId = requestAnimationFrame(detectFrame);
            }, isDetecting ? 50 : 100);
        }

        // Button event listeners
        pauseBtn.addEventListener('click', () => {
            isDetecting = !isDetecting;
            pauseBtn.textContent = isDetecting ? 'Pause Detection' : 'Resume Detection';
            statusDesc.textContent = isDetecting ? 'High-precision detection resumed' : 'Detection paused';
            
            if (isDetecting && !animationId) {
                detectFrame();
            }
        });

        resetBtn.addEventListener('click', () => {
            totalWeeds = 0;
            totalCrops = 0;
            weedId = 0;
            cropId = 0;
            seenObjects.clear();
            objectLabels.clear();
            
            weedCount.textContent = '0';
            cropCount.textContent = '0';
            totalCount.textContent = '0';
            confidenceFill.style.width = '0%';
            confidenceLabel.textContent = 'Avg Confidence: 0%';
            statusDesc.textContent = 'Counts reset - High-precision detection active';
            
            ctx.clearRect(0, 0, canvas.width, canvas.height);
        });

        // Draggable panel functionality
        let startY = 0;
        let startHeight = 0;
        let dragging = false;
        const minHeight = 100;
        const maxHeight = 400;

        dragHandle.addEventListener('mousedown', startDrag);
        dragHandle.addEventListener('touchstart', startDrag);

        function startDrag(e) {
            dragging = true;
            startY = e.type === 'mousedown' ? e.clientY : e.touches[0].clientY;
            startHeight = bottomWrapper.offsetHeight;
            dragHandle.style.cursor = 'grabbing';
            
            document.addEventListener('mousemove', doDrag);
            document.addEventListener('touchmove', doDrag);
            document.addEventListener('mouseup', stopDrag);
            document.addEventListener('touchend', stopDrag);
            
            e.preventDefault();
        }

        function doDrag(e) {
            if (!dragging) return;
            
            const currentY = e.type === 'mousemove' ? e.clientY : e.touches[0].clientY;
            const deltaY = startY - currentY;
            let newHeight = startHeight + deltaY;
            
            // Constrain height
            newHeight = Math.max(minHeight, Math.min(maxHeight, newHeight));
            
            bottomWrapper.style.height = `${newHeight}px`;
            e.preventDefault();
        }

        function stopDrag() {
            dragging = false;
            dragHandle.style.cursor = 'grab';
            document.removeEventListener('mousemove', doDrag);
            document.removeEventListener('touchmove', doDrag);
            document.removeEventListener('mouseup', stopDrag);
            document.removeEventListener('touchend', stopDrag);
        }

        // Initialize
        startCamera();
    </script>
</body>
</html>
